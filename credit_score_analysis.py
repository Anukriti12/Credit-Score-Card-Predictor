# -*- coding: utf-8 -*-
"""Credit_Score_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eTANZHLa0i2GZ5vmn9cQcZ75xJXJnb7_
"""



# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn as sklearn
import seaborn as sns
# %matplotlib inline
from scipy import stats

"""# (1) Data Preprocessing"""

data = pd.read_csv("CreditScoring(After_Preprocessing).csv")
data.head()

## Dataset Dimensions
data.shape

"""### Questions to keep in mind:
#### (1) check maximum and minimum values
#### (2) check the scale of the variables (categorical, continuous, etc)
#### (3) look for outliers, weird values, missing values
"""

# Checking if any Missing Values are there in the Dataset
# No Missing Values are there in the Dataset
data.isnull().any()

## checking if any categorical Features are there in the Dataset
categorical_data = data.select_dtypes(exclude=[np.number])
print ("There are {} categorical Columns in Dataset".format(categorical_data.shape[1]))

# Name of all the Categorical Features Present in the Dataset
categorical_data.any()

from sklearn.preprocessing import LabelEncoder
model = LabelEncoder()
data['Status'] = model.fit_transform(data['Status'].astype('str'))
data['Home'] = model.fit_transform(data['Home'].astype('str'))
data['Marital'] = model.fit_transform(data['Marital'].astype('str'))
data['Job'] = model.fit_transform(data['Job'].astype('str'))
data['Records'] = model.fit_transform(data['Records'].astype('str'))

data.head()

#Checking Data Types of the Features for Confirmation
data.dtypes

"""## (2) Data Exploration"""

# Summary of the Data
data.describe()

## Value Counts of 'GOOD' Status and 'BAD' Status
## 'GOOD': 3197 and 'BAD': 1249
data.Status.value_counts()

# As per the Dataset there are approximately 72% GOOD Score and 28% BAD Score instances
status_count = data.Status.value_counts()/len(data)
status_count

"""## PEARSON’S CORRELATION
#### Covariance is rarely used in summary statistics because it is hard to interpret. By itself, it does not provide a sense of how much the two variables vary together, but only their ‘direction’ (if you consider each series to be a vector). The unit of Covariance is also confusing because it is the product of two different units. Pearson’s Correlation divides the Covariance with the product of the standard deviations of both series resulting in a dimensionless value.

#### Pearson’s Correlation is bounded in [-1, 1].
![Correlation Formula](files/corr.jpg "Correlation")
#### If it is positive, the two variables tend to be high or low together
#### If it is negative, the two variables tend to be opposite of each other
#### If it is zero or close to zero they don’t affect each other
#### Sx, Sy are the standard deviations of the X, Y series respectively. Standard Deviation (σ) is a measure of the spread of the distribution.
"""

# Correlation between all the Features
# Correlation Plot
corr = data.corr()
sns.heatmap(corr)
corr

"""#### As per the Correlation Matrix and the Correlation Values Calculated we can say that No Feature in the Dataset is highly Correlated with the Target Attribute 'Status'. Hence, there is no need for Feature Selection as of now and the Classification Model Such as 'Linear Regression ' and 'Logistic Regression' etc, can be performed directly on all the Features of the Dataset."""

## Correlation Values of all the Features with respect to Target Variable 'Status' 
## Top 10 Values
print (corr['Status'].sort_values(ascending=False)[:10], '\n')

## Last 5 Values
print (corr['Status'].sort_values(ascending=False)[-5:])

## Visualising Correlation Matrix with actual Correlation Values
cmap=sns.diverging_palette(5, 250, as_cmap=True)

def magnify():
    return [dict(selector="th",
                 props=[("font-size", "7pt")]),
            dict(selector="td",
                 props=[('padding', "0em 0em")]),
            dict(selector="th:hover",
                 props=[("font-size", "15pt")]),
            dict(selector="tr:hover td:hover",
                 props=[('max-width', '200px'),
                        ('font-size', '15pt')])
]

corr.style.background_gradient(cmap, axis=1)\
    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\
    .set_caption("Hover to magify")\
    .set_precision(3)\
    .set_table_styles(magnify())

"""##### Dummy variables are "proxy" variables or numeric stand-ins for qualitative facts in a regression model. In regression analysis, the dependent variables may be influenced not only by quantitative variables (income, output, prices, etc.), but also by qualitative variables (gender, religion, geographic region, etc.). A dummy independent variable (also called a dummy explanatory variable) which for some observation has a value of 0 will cause that variable's coefficient to have no role in influencing the dependent variable, while when the dummy takes on a value 1 its coefficient acts to alter the intercept. For example, suppose membership in a group is one of the qualitative variables relevant to a regression. If group membership is arbitrarily assigned the value of 1, then all others would get the value 0.

## Histogram Plots of all the Features in the Dataset
"""

num_bins = 10

data.hist(bins = num_bins, figsize=(20,15))
plt.savefig("Data_Histogram_Plots")
# plt.show()

"""## Scatterplots"""

columns = ['Status', 'Seniority', 'Home', 'Time', 'Age', 'Marital', 'Records', 'Job', 'Expenses', 'Income', 'Assets', 'Debt', 'Amount', 'Price', 'Finrat', 'Savings']
data.plot.scatter(x=columns[0], y=columns[14])
data.plot.hexbin(x=columns[0], y=columns[14], gridsize=10)
data.plot.scatter(x=columns[0], y=columns[15])
data.plot.hexbin(x=columns[0], y=columns[15], gridsize=25)
plt.show()

"""## Creating Dummy Variables"""

list( data.columns )

X = list( data.columns )
X.remove( 'Status' )
X

Y = data['Status']

credit_data = pd.get_dummies( data[X], drop_first = True )
len( credit_data.columns )

### Splitting Dataset into 'Training' and 'Testing' Dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( data[X], Y, test_size = 0.3, random_state = 42 )

X_test

abc=sm.add_constant( X_test )

abc

"""## (3) Logistic Regression Model"""

## Using 'statsmodel.api' you can use R-style formulas together with pandas data frames to fit your models
import statsmodels.api as sm
logit = sm.Logit( y_train, sm.add_constant( X_train ) )
lg = logit.fit()

import pickle

# pickle.dump(logit, open('model.pkl','wb'))

pickle.dump(lg, open('model1.pkl','wb'))
model1=pickle.load(open('model1.pkl','rb'))

# model=pickle.load(open('model.pkl','rb'))

import pandas as pd
data = [[1.0,3,3,60,26,3,0,0,35,83,0,0,1050,1156,90.830450,2.742857]]
df = pd.DataFrame(data,columns=['const','Seniority','Home','Time','Age','Marital','Records','Job','Expenses','Income','Assets','Debt','Amount','Price','Finrat','Savings'])
print(df)

df.shape

import numpy as np
int_features = [1.0,3,3,60,26,3,0,0,35,83,0,0,1050,1156,90.830450,2.742857]
final_features = [np.array(int_features)]
prediction = model1.predict( sm.add_constant(final_features))
#[0].predicted_prob.map( lambda x: 1 if x > 0.5 else 0)

prediction[0]

sd=sm.add_constant(df)

sd.shape

sd

def get_predictions(lg):
    y_pred_df = pd.DataFrame( { "predicted_prob": lg.predict( sm.add_constant(df) ) } )
    return y_pred_df

y_pred_df = get_predictions(model1)

y_pred_df['predicted'] = y_pred_df.predicted_prob.map( lambda x: 1 if x > 0.5 else 0)

y_pred_df

ans = y_pred_df['predicted']
ans



# Seniority	Home	Time	Age	Marital	Records	Job	Expenses	Income	Assets	Debt	Amount	Price	Finrat	Savings
# 3286	3	3	60	26	3	0	0	35	83	0	0	1050	1156	90.830450	2.742857

X_test.shape

lg.summary()

"""## Find Significant Variables"""

def get_significant_vars( lm ):
    var_p_vals_df = pd.DataFrame( lm.pvalues )
    var_p_vals_df['vars'] = var_p_vals_df.index
    var_p_vals_df.columns = ['pvals', 'vars']
    return list( var_p_vals_df[var_p_vals_df.pvals <= 0.05]['vars'] )

significant_vars = get_significant_vars( lg )

lg

"""### The Significant Features in the Dataset obtained are ['Seniority', 'Home', 'Marital', 'Records', 'Job', 'Expenses', 'Income', 'Assets', 'Debt', 'Finrat']"""

significant_vars

"""### Testing the Model and measuring Accuracy Score"""

from sklearn import metrics

X_test.shape

def get_predictions( y_test, model ):
    y_pred_df = pd.DataFrame( { 'actual': y_test,
                               "predicted_prob": lg.predict( sm.add_constant( X_test ) ) } )
    return y_pred_df

y_pred_df = get_predictions( y_test, lg )

## Calculating predicted probability for the Target Variable 'Status(Default Classes)'
y_pred_df[0:10]

y_pred_df['predicted'] = y_pred_df.predicted_prob.map( lambda x: 1 if x > 0.5 else 0)

y_pred_df[0:10]

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pylab as plt
# %matplotlib

def draw_cm( actual, predicted ):
    cm = metrics.confusion_matrix( actual, predicted, [1,0] )
    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = ["No Default", "Default"] , yticklabels = ["No Default", "Default"] )
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

draw_cm( y_pred_df.actual, y_pred_df.predicted )

## Finding Overall Accuracy of the Model
print( 'Total Accuracy : ',np.round( metrics.accuracy_score( y_test, y_pred_df.predicted ), 2 ) )
print( 'Precision : ',np.round( metrics.precision_score( y_test, y_pred_df.predicted ), 2 ) )
print( 'Recall : ',np.round( metrics.recall_score( y_test, y_pred_df.predicted ), 2 ) )

cm1 = metrics.confusion_matrix( y_pred_df.actual, y_pred_df.predicted, [1,0] )

sensitivity = cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Sensitivity : ', round( sensitivity, 2) )

specificity = cm1[1,1]/(cm1[1,0]+cm1[1,1])
print('Specificity : ', round( specificity, 2 ) )

"""### Predicted Probability distribution Plots for Defaults(BAD) and Non Defaults(GOOD)"""

sns.distplot( y_pred_df[y_pred_df.actual == 1]["predicted_prob"], kde=False, color = 'b' )
sns.distplot( y_pred_df[y_pred_df.actual == 0]["predicted_prob"], kde=False, color = 'g' )

auc_score = metrics.roc_auc_score( y_pred_df.actual, y_pred_df.predicted_prob  )
round( float( auc_score ), 2 )

"""### Plotting ROC Curve"""

def draw_roc( actual, probs ):
    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,
                                              drop_intermediate = False )
    auc_score = metrics.roc_auc_score( actual, probs )
    plt.figure(figsize=(6, 4))
    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()

    return fpr, tpr, thresholds

fpr, tpr, thresholds = draw_roc( y_pred_df.actual, y_pred_df.predicted_prob )

"""### Finding Optimal Cutt-off Probability"""

thresholds[0:10]

fpr[0:10]

tpr[0:10]

"""## Find Optimal Cut-off Using YOUDEN's INDEX
#### Youden's J statistic (also called Youden's index) is a single statistic that captures the performance of a dichotomous diagnostic test. Informedness is its generalization to the multiclass case and estimates the probability of an informed decision.
![Youden's Index Formula](files/Capture.PNG "YOUDEN's INDEX")
#### That is when (TPR+TNR -1) is maximum. || max( TPR - (1 - TNR) ) || max( TPR - FPR )
"""

tpr_fpr = pd.DataFrame( { 'tpr': tpr, 'fpr': fpr, 'thresholds': thresholds } )
tpr_fpr['diff'] = tpr_fpr.tpr - tpr_fpr.fpr
tpr_fpr.sort_values( 'diff', ascending = False )[0:10]

y_pred_df['predicted_new'] = y_pred_df.predicted_prob.map( lambda x: 1 if x > 0.29 else 0)

draw_cm( y_pred_df.actual, y_pred_df.predicted_new )

"""## Finding Optimal Cut-off Probability using Cost"""

cm = metrics.confusion_matrix( y_pred_df.actual, y_pred_df.predicted_new, [1,0] )

cm_mat = np.array( cm )

cm_mat[1, 0]

cm_mat[0, 1]

def get_total_cost( actual, predicted ):
    cm = metrics.confusion_matrix( actual, predicted, [1,0] )
    cm_mat = np.array( cm )
    return cm_mat[0,1] * 2 + cm_mat[0,1] * 1

get_total_cost( y_pred_df.actual, y_pred_df.predicted_new )

cost_df = pd.DataFrame( columns = ['prob', 'cost'])

idx = 0
for each_prob in range( 20, 50):
    cost = get_total_cost( y_pred_df.actual,
                          y_pred_df.predicted_prob.map(
            lambda x: 1 if x > (each_prob/100)  else 0) )
    cost_df.loc[idx] = [(each_prob/100), cost]
    idx += 1

cost_df.sort_values( 'cost', ascending = True )[0:5]

y_pred_df['predicted_final'] = y_pred_df.predicted_prob.map( lambda x: 1 if x > 0.20 else 0)

draw_cm( y_pred_df.actual, y_pred_df.predicted_final )

"""### The Accuracy of the Model got Reduced but the Quadrants that Contribute to the Cost are Minimised"""

print( 'Total Accuracy : ',np.round( metrics.accuracy_score( y_test, y_pred_df.predicted_final ), 2 ) )
print( 'Precision : ',np.round( metrics.precision_score( y_test, y_pred_df.predicted_final ), 2 ) )
print( 'Recall : ',np.round( metrics.recall_score( y_test, y_pred_df.predicted_final ), 2 ) )

cm1 = metrics.confusion_matrix( y_pred_df.actual, y_pred_df.predicted_final, [1,0] )

sensitivity = cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Sensitivity : ', round( sensitivity, 2) )

specificity = cm1[1,1]/(cm1[1,0]+cm1[1,1])
print('Specificity : ', round( specificity, 2 ) )

"""## (4) Decision Tree Model"""

